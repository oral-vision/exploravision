History
	Oral cavity cancers are among the earliest malignancies documented in human history. Ancient medical texts from Egypt and India (c. 3000 BCE) describe ulcerative lesions of the mouth that were resistant to treatment and often fatal. The Greek philosopher Hippocrates later classified such growths as karkinos (cancer), recognizing their invasive and destructive nature. However, early physicians lacked both the anatomical knowledge and diagnostic tools necessary to distinguish malignant lesions from infections or trauma.
	During the Middle Ages and Renaissance, oral cancers were primarily diagnosed by gross visual inspection and palpation. Lesions that were persistent, ulcerated, indurated, or bleeding were considered spiritually ominous. Treatment was largely surgical and rudimentary, often involving cauterization or excision without anesthesia. Thus, prognosis for oral cavity cancers was quite poor, largely due to late detection and the absence of standardized diagnostic criteria.
	However, a major conceptual shift occurred in the 18th and 19th centuries, when pathological anatomy and microscopy emerged. Italian anatomist Giovanni Battista Morgagni and later German physician Rudolf Virchow established that cancers arose from abnormal cells rather than imbalances of bodily humors (blood, yellow bile, black bile, and phlegm). This period marked the beginning of histopathology, which would become the gold standard for cancer diagnosis for most of the modern era.
	By the early 20th century, oral cavity cancers like oral squamous cell carcinoma (OSCC) were recognized as distinct clinical entities.  Epidemiological studies began linking oral cancer to tobacco use, including cigarette smoking and smokeless tobacco consumption. Meanwhile, in South and Southeast Asia, betel nut (areca nut) chewing was identified as a major carcinogenic risk factor. Subsequently, scalpel biopsy procedures were rapidly adopted worldwide, building upon previous advances in histopathology to aid in the diagnosis of oral cavity cancers.

	In the late 20th century, new diagnostic methods that were relatively less invasive than their predecessors were developed. Oral brush biopsies became widely used in place of invasive scalpel biopsies to collect dysplastic cells for histopathological analysis. More recently, fluorescence-based-imaging has been tested for the diagnosis of oral lesions, indicating a shift from histopathology to optical analysis.
Future Technology
OralVision is a potential intraoral diagnostic system made up of two main components: an imaging device and an image processing unit. The control center for OralVision’s imaging device would be a XIAO-ESP32-C6 microcontroller, a cheap, yet powerful solution to reduce production costs while maintaining the technical capabilities of larger, more expensive controllers. The image would be taken using an OV5642 5MP image sensor with SPI compatibility for high-bandwidth data transfer to the microcontroller. In addition, an anti-fog nano coating would be applied to the camera lens to prevent moisture in the mouth from harming the internal components. To minimize user error, the entire system would use a single medical-grade momentary push button to control image streaming and capturing. The power would be regulated using a 5V voltage regulator that accepts multiple power sources, including USB-C (with a port for easy charging access), AA batteries, and solar power (for extremely low-income areas with minimal electrical infrastructure). The device would also ship with rechargeable nickel-metal hydride batteries to prevent thermal runaway during transportation. The entire imaging system would ultimately be encased in a clinical sterilization wrap to prevent bacteria and microbes from entering the device, with the prototype’s housing being held together using a snap-fit mechanism for easy access to the electronics for battery replacements and software updates. These components are visualized by the CAD drawings found in Figures 1 and 2.
Figure 1. CAD Drawing of OralVision’s Imaging Device

Figure 2. Feature Notations of OralVision Imaging Device
For communication protocols between the imaging device and the machine learning interface, we plan to use SPI and Wi-Fi (see Figure 3). We chose SPI over UART and I2C because of its ability to transmit high-resolution intraoral images continuously with minimal latency. The device transfers images to our ML interface wirelessly via Wi-Fi 5, where our decentralized federated learning model processes the images through computer vision and transmits diagnostic results and Bayesian inferences to the screen on our Raspberry Pi 5 human-machine interface (HMI). When a device processes an image, it systematically alters neural parameters to prevent overfitting and improve the accuracy of our main model, which clients will be able to download from our website periodically for software updates.
Figure 3. Current OralVision Software Architecture Overview
 
We envision that this deep-learning-based diagnostic technology will revolutionize the market for oral diagnostics in the next decade, especially with the recent advent of novel computer vision models, by providing a far greater degree of accuracy than current market competitors like the VELScope Vx and the ViziLite PRO, which ultimately rely on optical analysis from human physicians, who have a greater tendency for misdiagnosis than a trained deep learning model, for the final diagnosis.

Breakthroughs
To make our proposed future oral cancer diagnostic device a reality, several key technological and scientific breakthroughs are required. The most critical breakthrough is the development of highly accurate, clinically-validated deep learning models capable of distinguishing early and late stage oral malignancies from benign oral conditions using noninvasive imaging alone. 
While AI has demonstrated promise in medical image classification, current models often struggle with generalizability due to limited, biased, or poorly annotated datasets. Achieving reliable performance across diverse populations, oral anatomies, and lesion types requires large-scale, standardized datasets and improved training methodologies that can learn subtle morphological and textural biomarkers associated with early malignant transformation. 
Another critical breakthrough involves being able to attain secure, privacy-preserving data sharing and model training frameworks, such as federated learning. Medical data fragmentation and strict privacy regulations like HIPAA severely limit the ability to train robust AI models on sufficiently large datasets. Thus, the incorporation of federated learning, a decentralized ML method, allows models to be trained across multiple spatially-disparate clinics without transferring sensitive patient data. Improvements in training stability, communication efficiency, and bias mitigation are required before such systems can be deployed reliably at scale.
This future diagnostic technology does not yet exist primarily because of data limitations, validation barriers, and integration challenges. Most existing publicly available oral lesion datasets are small, inconsistently labeled, and collected using nonstandardized imaging conditions, making it difficult to train AI models that generalize well in real-world settings. Additionally, early-stage oral cancers are relatively rare compared to benign lesions, creating class imbalance issues that reduce model reliability.
Furthermore, regulatory and ethical concerns, particularly around AI decision-making in healthcare, have slowed translation from research prototypes to approved clinical tools significantly. Without rigorous prospective validation and explainability, AI-driven diagnostics face significant hurdles to adoption.
Ultimately, the most essential breakthrough for the success of this future technology is the creation of a robust deep learning model capable of objectively classifying oral lesions from intraoral images with high sensitivity and specificity.
A research project to address this breakthrough would involve collecting a large dataset of standardized intraoral images from patients presenting with a wide range of oral lesions. Images would be captured under controlled lighting conditions using the prototype imaging device. Each lesion would be independently labeled based on histopathological biopsy results, which serve as the clinical gold standard. The dataset would then be divided into training, validation, and external test cohorts to assess generalizability.
The deep learning model, which would most likely be a federated convolutional neural network (F–CNN), would be trained to extract spatial, textural, and color-based features associated with malignancy. Performance would be evaluated using quantitative metrics including sensitivity, specificity, diagnostic accuracy, receiver operating characteristic area under the curve (ROC–AUC), and false-positive rate. Then, additional comparative analyses would be performed to see how OralVision competes against current technologies like the VELScope and ViziLite systems.