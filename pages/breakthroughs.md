---
layout: page
title: Breakthroughs
description: More on how we're revolutionizing oral diagnostics in underserved communities
image: assets/images/solution.jpg
show-tile: true
hide_image: true
tile_order: 3
menu_order: 3
nav-menu: true
---
## Deep Learning Models

The most critical breakthrough is the development of highly accurate, clinically-validated deep learning models capable of distinguishing early and late stage oral malignancies from benign oral conditions using noninvasive imaging alone.

While AI has demonstrated promise in medical image classification, current models often struggle with generalizability due to limited, biased, or poorly annotated datasets. Achieving reliable performance across diverse populations, oral anatomies, and lesion types requires large-scale, standardized datasets and improved training methodologies that can learn subtle morphological and textural biomarkers associated with early malignant transformation.

## Privacy-Preserving Training

Another critical breakthrough involves being able to attain secure, privacy-preserving data sharing and model training frameworks, such as federated learning. Medical data fragmentation and strict privacy regulations like HIPAA severely limit the ability to train robust AI models on sufficiently large datasets.

Thus, the incorporation of federated learning, a decentralized ML method, allows models to be trained across multiple spatially-disparate clinics without transferring sensitive patient data. Improvements in training stability, communication efficiency, and bias mitigation are required before such systems can be deployed reliably at scale.

## Previous Barriers to Invention 

This future diagnostic technology does not yet exist primarily because of data limitations, validation barriers, and integration challenges.

| Barrier / challenge              | Impact                                                                 |
|----------------------------------|------------------------------------------------------------------------|
| Small, inconsistently labeled datasets | Difficult to train AI models that generalize well in real-world settings |
| Nonstandardized imaging conditions    | Same as above                                                          |
| Class imbalance (early cancers rare)  | Reduces model reliability                                              |
| Regulatory and ethical concerns       | Slowed translation from research prototypes to approved clinical tools  |

Most existing publicly available oral lesion datasets are small, inconsistently labeled, and collected using nonstandardized imaging conditions, making it difficult to train AI models that generalize well in real-world settings. Additionally, early-stage oral cancers are relatively rare compared to benign lesions, creating class imbalance issues that reduce model reliability.

Furthermore, regulatory and ethical concerns, particularly around AI decision-making in healthcare, have slowed translation from research prototypes to approved clinical tools significantly. Without rigorous prospective validation and explainability, AI-driven diagnostics face significant hurdles to adoption.

<!-- ## Most Essential Breakthrough

Ultimately, the most essential breakthrough for the success of this future technology is the creation of a robust deep learning model capable of objectively classifying oral lesions from intraoral images with high sensitivity and specificity.

## Research Project to Address This Breakthrough

A research project to address this breakthrough would involve collecting a large dataset of standardized intraoral images from patients presenting with a wide range of oral lesions. Images would be captured under controlled lighting conditions using the prototype imaging device. Each lesion would be independently labeled based on histopathological biopsy results, which serve as the clinical gold standard. The dataset would then be divided into training, validation, and external test cohorts to assess generalizability.

The deep learning model, which would most likely be a federated convolutional neural network (F–CNN), would be trained to extract spatial, textural, and color-based features associated with malignancy. Performance would be evaluated using quantitative metrics including sensitivity, specificity, diagnostic accuracy, receiver operating characteristic area under the curve (ROC–AUC), and false-positive rate. Then, additional comparative analyses would be performed to see how OralVision competes against current technologies like the VELScope and ViziLite systems. -->
